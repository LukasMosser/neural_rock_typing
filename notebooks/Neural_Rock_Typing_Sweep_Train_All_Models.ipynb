{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sweep Neural Rock Typing - Train Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcFFZDhDCy3E"
      },
      "source": [
        "## Neural Rock Train Model Notebook\n",
        "\n",
        "The following cell sets up the entire repository from githubg and links to the google drive where the dataset it stored. After all the requirements get installed."
      ],
      "id": "qcFFZDhDCy3E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "institutional-attribute",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1a5ee2-e904-40cd-f077-100f124b4688"
      },
      "source": [
        "import os\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    import os\n",
        "    from getpass import getpass\n",
        "    import urllib\n",
        "\n",
        "    user = input('User name: ')\n",
        "    password = getpass('Password: ')\n",
        "    password = urllib.parse.quote(password) # your password is converted into url format\n",
        "\n",
        "    cmd_string = 'git clone https://{0}:{1}@github.com/LukasMosser/neural_rock_typing.git'.format(user, password)\n",
        "\n",
        "    os.system(cmd_string)\n",
        "    cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "    os.chdir(\"./neural_rock_typing\")\n",
        "    os.system('pip install -r requirements.txt')\n",
        "    os.system('pip install -e .')\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ],
      "id": "institutional-attribute",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n",
            "User name: LukasMosser\n",
            "Password: ··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFfg8b4uUu9Z"
      },
      "source": [
        "import os\n",
        "os.chdir(\"./neural_rock_typing\")"
      ],
      "id": "hFfg8b4uUu9Z",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO2loavbW0WL"
      },
      "source": [
        "### A Hack needed to make Pytorch Lightning work with Colab again"
      ],
      "id": "xO2loavbW0WL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91wcJjWX6cm8",
        "outputId": "a3c4f7e7-bc67-4214-aa35-fc4ff1d8d79b"
      },
      "source": [
        "!pip install wandb\n",
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "import pytorch_lightning as pl"
      ],
      "id": "91wcJjWX6cm8",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.30)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.1)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
            "Collecting git+https://github.com/PyTorchLightning/pytorch-lightning\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning to /tmp/pip-req-build-h5ygrrci\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-req-build-h5ygrrci\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): pytorch-lightning==1.4.0.dev0 from git+https://github.com/PyTorchLightning/pytorch-lightning in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (2021.4.0)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (0.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (20.9)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (1.8.1+cpu)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (0.18.2)\n",
            "Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (0.3.0)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (2.4.1)\n",
            "Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (3.7.4.post0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning==1.4.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning==1.4.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.36.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (56.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.28.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (20.3.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (3.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.4.1)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.4.0.dev0-cp37-none-any.whl size=807126 sha256=e1b207239be660f81f0ac83f066b16656d58c59718bd0bf31c361478b61e9b9e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5lfz_9my/wheels/e2/c6/88/caa5d4cfbfab631fc84b0107896a6f661a1caf589160c27e71\n",
            "Successfully built pytorch-lightning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sW6uB2zCxoq"
      },
      "source": [
        "## Login to Weights & Biases for Logging"
      ],
      "id": "3sW6uB2zCxoq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA58DQTTmfbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb72664-9082-45c4-8ee2-99434fe236c7"
      },
      "source": [
        "!wandb login"
      ],
      "id": "iA58DQTTmfbc",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukas-mosser\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5l4qTILU6_J",
        "outputId": "a4c3c93f-91ce-46b6-a78c-c161d5ba396a"
      },
      "source": [
        "!git pull"
      ],
      "id": "H5l4qTILU6_J",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "second-monitoring"
      },
      "source": [
        "## Basic Imports"
      ],
      "id": "second-monitoring"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alpine-hanging"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import json\n",
        "import wandb\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "from neural_rock.dataset import GPUThinSectionDataset\n",
        "from neural_rock.model import NeuralRockModel, make_vgg11_model, make_resnet18_model\n",
        "from neural_rock.plot import visualize_batch\n",
        "from neural_rock.utils import MEAN_TRAIN, STD_TRAIN"
      ],
      "id": "alpine-hanging",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "occasional-impression"
      },
      "source": [
        "## Hyperparameters"
      ],
      "id": "occasional-impression"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_8WWBgPjO5"
      },
      "source": [
        "wandb_name = 'lukas-mosser'\n",
        "learning_rate = 3e-4\n",
        "batch_size = 16\n",
        "weight_decay = 1e-5\n",
        "dropout = 0.5\n",
        "\n",
        "train_dataset_mult = 50\n",
        "val_dataset_mult = 50\n",
        "\n",
        "seed = 42"
      ],
      "id": "NX_8WWBgPjO5",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fqZTGkOCtAV"
      },
      "source": [
        "## Perform Training Sweep across 12 Models\n",
        "\n",
        "We train a Resnet and a VGG network each with a frozen feature extractor for each labelset: Lucia, Dunham, and DominantPore Type. \n",
        "\n",
        "This leads to a total of 12 models."
      ],
      "id": "9fqZTGkOCtAV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "0C6Tp6_ag5sq",
        "outputId": "d0c52c66-29f6-440f-d4bb-5b2904c36feb"
      },
      "source": [
        "for labelset in ['Lucia', 'Dunham', 'DominantPore']:\n",
        "  for model in ['vgg', 'resnet']:\n",
        "    for frozen in [True, False]:\n",
        "\n",
        "      # Set the base path for the models to be stored in the Google Drive\n",
        "      path = Path(\"./data/models/{0:}/{1:}/{2:}\".format(labelset, model, str(frozen)))\n",
        "      path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "      # Set the Random Seed on Everything\n",
        "      pl.seed_everything(seed)\n",
        "\n",
        "      # Data Augmentation used for Training\n",
        "      data_transforms = {\n",
        "          'train': transforms.Compose([\n",
        "              transforms.RandomHorizontalFlip(),\n",
        "              transforms.RandomRotation(degrees=360),\n",
        "              transforms.RandomCrop((512, 512)),\n",
        "              transforms.ColorJitter(hue=0.5),\n",
        "              transforms.Resize((224, 224)),\n",
        "              transforms.Normalize(mean=MEAN_TRAIN, std=STD_TRAIN)\n",
        "          ]),\n",
        "          'val':\n",
        "              transforms.Compose([\n",
        "                  transforms.RandomCrop((512, 512)),\n",
        "                  transforms.Resize((224, 224)),\n",
        "                  transforms.Normalize(mean=MEAN_TRAIN, std=STD_TRAIN)\n",
        "              ])\n",
        "      }\n",
        "\n",
        "      # Load the Datasets\n",
        "      train_dataset_base = GPUThinSectionDataset(Path(\".\"), labelset, preload_images=True,\n",
        "                                          transform=data_transforms['train'], train=True, seed=seed)\n",
        "\n",
        "      val_dataset_base = GPUThinSectionDataset(Path(\".\"), labelset, preload_images=True,\n",
        "                                          transform=data_transforms['train'], train=False, seed=seed)\n",
        "      train_test_split = {'train': train_dataset_base.image_ids, 'test': val_dataset_base.image_ids}\n",
        "      \n",
        "      with open(path.joinpath('train_test_split.json'), 'w') as fp:\n",
        "          json.dump(train_test_split, fp)\n",
        " \n",
        "      # We multiply the validation dataset to randomly increase the number of images we evaluate against.\n",
        "      val_dataset = ConcatDataset([val_dataset_base]*100)\n",
        "\n",
        "      # Setup dataloaders\n",
        "      train_loader = DataLoader(train_dataset_base, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
        "      val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "      # Setup Weights and Biases Logger\n",
        "      wandb_logger = WandbLogger(name=wandb_name, project='neural-rock-finak-3', entity='ccg')\n",
        "      wandb_logger.experiment.config.update({\"labelset\": labelset, \"model\": model, 'frozen': str(frozen)})\n",
        "      tensorboard_logger = TensorBoardLogger(\"lightning_logs\", name=labelset)\n",
        "      \n",
        "      # Checkpoint based on validation F1 score\n",
        "      checkpointer = ModelCheckpoint(dirpath=path, filename='best', monitor=\"val/f1\", verbose=True, mode=\"max\")\n",
        "      \n",
        "      # Setup the Pytorch Lightning Dataloader\n",
        "      trainer = pl.Trainer(gpus=-1, \n",
        "                           max_steps=15000, \n",
        "                           benchmark=True,\n",
        "                          logger=[wandb_logger, tensorboard_logger],\n",
        "                          callbacks=[checkpointer],\n",
        "                          progress_bar_refresh_rate=20,\n",
        "                          check_val_every_n_epoch=100)\n",
        "      \n",
        "      # Select which model to run\n",
        "      if model == 'vgg':\n",
        "        feature_extractor, classifier = make_vgg11_model(train_dataset_base.num_classes, dropout=dropout)\n",
        "      elif model == 'resnet':\n",
        "        feature_extractor, classifier = make_resnet18_model(train_dataset_base.num_classes)\n",
        "\n",
        "      # Create the model itself, ready for training\n",
        "      model_ = NeuralRockModel(feature_extractor, classifier, num_classes=train_dataset_base.num_classes, freeze_feature_extractor=frozen)\n",
        "\n",
        "      # Train the model\n",
        "      trainer.fit(model_, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "      # Clean up some images on GPU to avoid Out of Memory errors\n",
        "      del train_dataset_base.images\n",
        "      del val_dataset_base.images\n",
        "\n",
        "      # Clean up Weights and Biases Logging\n",
        "      wandb.finish()"
      ],
      "id": "0C6Tp6_ag5sq",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-36ee338e2987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       train_dataset_base = GPUThinSectionDataset(Path(\".\"), labelset, preload_images=True,\n\u001b[0;32m---> 27\u001b[0;31m                                           transform=data_transforms['train'], train=True, seed=seed)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       val_dataset_base = GPUThinSectionDataset(Path(\".\"), labelset, preload_images=True,\n",
            "\u001b[0;32m/content/neural_rock_typing/neural_rock/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPUThinSectionDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gpu_preload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gpu_preload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/neural_rock_typing/neural_rock/dataset.py\u001b[0m in \u001b[0;36m_gpu_preload_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_as_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0msample_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sample_labels_as_class_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             raise AssertionError(\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBdnzL60VLzp"
      },
      "source": [
        ""
      ],
      "id": "wBdnzL60VLzp",
      "execution_count": null,
      "outputs": []
    }
  ]
}