{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "qcFFZDhDCy3E",
   "metadata": {
    "id": "qcFFZDhDCy3E"
   },
   "source": [
    "## Neural Rock Train Model Notebook\n",
    "\n",
    "The following cell sets up the entire repository from githubg and links to the google drive where the dataset it stored. After all the requirements get installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "institutional-attribute",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "institutional-attribute",
    "outputId": "9e1a5ee2-e904-40cd-f077-100f124b4688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    import os\n",
    "    from getpass import getpass\n",
    "    import urllib\n",
    "\n",
    "    user = input('User name: ')\n",
    "    password = getpass('Password: ')\n",
    "    password = urllib.parse.quote(password) # your password is converted into url format\n",
    "\n",
    "    cmd_string = 'git clone https://{0}:{1}@github.com/LukasMosser/neural_rock_typing.git'.format(user, password)\n",
    "\n",
    "    os.system(cmd_string)\n",
    "    cmd_string, password = \"\", \"\" # removing the password from the variable\n",
    "    os.chdir(\"./neural_rock_typing\")\n",
    "    os.system('pip install -r requirements.txt')\n",
    "    os.system('pip install -e .')\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "else:\n",
    "    print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hFfg8b4uUu9Z",
   "metadata": {
    "id": "hFfg8b4uUu9Z"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './neural_rock_typing'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-b7496d86e652>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"./neural_rock_typing\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './neural_rock_typing'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"./neural_rock_typing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xO2loavbW0WL",
   "metadata": {
    "id": "xO2loavbW0WL"
   },
   "source": [
    "### A Hack needed to make Pytorch Lightning work with Colab again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91wcJjWX6cm8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91wcJjWX6cm8",
    "outputId": "a3c4f7e7-bc67-4214-aa35-fc4ff1d8d79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.30)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.1.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
      "Collecting git+https://github.com/PyTorchLightning/pytorch-lightning\n",
      "  Cloning https://github.com/PyTorchLightning/pytorch-lightning to /tmp/pip-req-build-h5ygrrci\n",
      "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-req-build-h5ygrrci\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "    Preparing wheel metadata ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytorch-lightning==1.4.0.dev0 from git+https://github.com/PyTorchLightning/pytorch-lightning in /usr/local/lib/python3.7/dist-packages\n",
      "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (4.41.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (2021.4.0)\n",
      "Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (0.3.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (20.9)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (1.8.1+cpu)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (1.19.5)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (0.18.2)\n",
      "Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (0.3.0)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.4.0.dev0) (2.4.1)\n",
      "Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (2.23.0)\n",
      "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (3.7.4.post0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning==1.4.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning==1.4.0.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.32.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.36.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (56.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.12.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.12.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning==1.4.0.dev0) (3.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (4.7.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.10.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.4.0.dev0) (3.4.1)\n",
      "Building wheels for collected packages: pytorch-lightning\n",
      "  Building wheel for pytorch-lightning (PEP 517) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.4.0.dev0-cp37-none-any.whl size=807126 sha256=e1b207239be660f81f0ac83f066b16656d58c59718bd0bf31c361478b61e9b9e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5lfz_9my/wheels/e2/c6/88/caa5d4cfbfab631fc84b0107896a6f661a1caf589160c27e71\n",
      "Successfully built pytorch-lightning\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64eb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3sW6uB2zCxoq",
   "metadata": {
    "id": "3sW6uB2zCxoq"
   },
   "source": [
    "## Login to Weights & Biases for Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iA58DQTTmfbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iA58DQTTmfbc",
    "outputId": "deb72664-9082-45c4-8ee2-99434fe236c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mlukas-mosser\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "H5l4qTILU6_J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5l4qTILU6_J",
    "outputId": "a4c3c93f-91ce-46b6-a78c-c161d5ba396a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-monitoring",
   "metadata": {
    "id": "second-monitoring"
   },
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alpine-hanging",
   "metadata": {
    "id": "alpine-hanging"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import wandb\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from neural_rock.dataset import GPUThinSectionDataset\n",
    "from neural_rock.model import NeuralRockModel, make_vgg11_model, make_resnet18_model\n",
    "from neural_rock.plot import visualize_batch\n",
    "from neural_rock.utils import MEAN_TRAIN, STD_TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-impression",
   "metadata": {
    "id": "occasional-impression"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "NX_8WWBgPjO5",
   "metadata": {
    "id": "NX_8WWBgPjO5"
   },
   "outputs": [],
   "source": [
    "wandb_name = 'lukas-mosser'\n",
    "learning_rate = 3e-4\n",
    "batch_size = 16\n",
    "weight_decay = 1e-5\n",
    "dropout = 0.5\n",
    "\n",
    "train_dataset_mult = 50\n",
    "val_dataset_mult = 50\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fqZTGkOCtAV",
   "metadata": {
    "id": "9fqZTGkOCtAV"
   },
   "source": [
    "## Perform Training Sweep across 12 Models\n",
    "\n",
    "We train a Resnet and a VGG network each with a frozen feature extractor for each labelset: Lucia, Dunham, and DominantPore Type. \n",
    "\n",
    "This leads to a total of 12 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e575c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0C6Tp6_ag5sq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "0C6Tp6_ag5sq",
    "outputId": "d0c52c66-29f6-440f-d4bb-5b2904c36feb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "for labelset in ['Lucia', 'Dunham', 'DominantPore']:\n",
    "  for model in ['vgg', 'resnet']:\n",
    "    for frozen in [True, False]:\n",
    "\n",
    "      # Set the base path for the models to be stored in the Google Drive\n",
    "      path = Path(\"./data/models/{0:}/{1:}/{2:}\".format(labelset, model, str(frozen)))\n",
    "      path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "      # Set the Random Seed on Everything\n",
    "      pl.seed_everything(seed)\n",
    "\n",
    "      # Data Augmentation used for Training\n",
    "      data_transforms = {\n",
    "          'train': transforms.Compose([\n",
    "              transforms.RandomHorizontalFlip(),\n",
    "              transforms.RandomRotation(degrees=360),\n",
    "              transforms.RandomCrop((512, 512)),\n",
    "              transforms.ColorJitter(hue=0.5),\n",
    "              transforms.Resize((224, 224)),\n",
    "              transforms.Normalize(mean=MEAN_TRAIN, std=STD_TRAIN)\n",
    "          ]),\n",
    "          'val':\n",
    "              transforms.Compose([\n",
    "                  transforms.RandomCrop((512, 512)),\n",
    "                  transforms.Resize((224, 224)),\n",
    "                  transforms.Normalize(mean=MEAN_TRAIN, std=STD_TRAIN)\n",
    "              ])\n",
    "      }\n",
    "\n",
    "      # Load the Datasets\n",
    "      train_dataset_base = GPUThinSectionDataset(Path(\".\"), labelset, preload_images=True,\n",
    "                                          transform=data_transforms['train'], train=True, seed=seed)\n",
    "\n",
    "      val_dataset_base = GPUThinSectionDataset(Path(\".\"), labelset, preload_images=True,\n",
    "                                          transform=data_transforms['train'], train=False, seed=seed)\n",
    "      train_test_split = {'train': train_dataset_base.image_ids, 'test': val_dataset_base.image_ids}\n",
    "      \n",
    "      with open(path.joinpath('train_test_split.json'), 'w') as fp:\n",
    "          json.dump(train_test_split, fp)\n",
    " \n",
    "      # We multiply the validation dataset to randomly increase the number of images we evaluate against.\n",
    "      val_dataset = ConcatDataset([val_dataset_base]*100)\n",
    "\n",
    "      # Setup dataloaders\n",
    "      train_loader = DataLoader(train_dataset_base, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "      val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n",
    "      print(len(train_loader), len(val_loader))\n",
    "      break\n",
    "      \"\"\"\n",
    "      # Setup Weights and Biases Logger\n",
    "      wandb_logger = WandbLogger(name=wandb_name, project='neural-rock-finak-3', entity='ccg')\n",
    "      wandb_logger.experiment.config.update({\"labelset\": labelset, \"model\": model, 'frozen': str(frozen)})\n",
    "      tensorboard_logger = TensorBoardLogger(\"lightning_logs\", name=labelset)\n",
    "      \n",
    "      # Checkpoint based on validation F1 score\n",
    "      checkpointer = ModelCheckpoint(dirpath=path, filename='best', monitor=\"val/f1\", verbose=True, mode=\"max\")\n",
    "      \n",
    "      # Setup the Pytorch Lightning Dataloader\n",
    "      trainer = pl.Trainer(gpus=-1, \n",
    "                           max_steps=15000, \n",
    "                           benchmark=True,\n",
    "                          logger=[wandb_logger, tensorboard_logger],\n",
    "                          callbacks=[checkpointer],\n",
    "                          progress_bar_refresh_rate=20,\n",
    "                          check_val_every_n_epoch=100)\n",
    "      \n",
    "      # Select which model to run\n",
    "      if model == 'vgg':\n",
    "        feature_extractor, classifier = make_vgg11_model(train_dataset_base.num_classes, dropout=dropout)\n",
    "      elif model == 'resnet':\n",
    "        feature_extractor, classifier = make_resnet18_model(train_dataset_base.num_classes)\n",
    "\n",
    "      # Create the model itself, ready for training\n",
    "      model_ = NeuralRockModel(feature_extractor, classifier, num_classes=train_dataset_base.num_classes, freeze_feature_extractor=frozen)\n",
    "\n",
    "      # Train the model\n",
    "      trainer.fit(model_, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "      # Clean up some images on GPU to avoid Out of Memory errors\n",
    "      del train_dataset_base.images\n",
    "      del val_dataset_base.images\n",
    "\n",
    "      # Clean up Weights and Biases Logging\n",
    "      wandb.finish()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wBdnzL60VLzp",
   "metadata": {
    "id": "wBdnzL60VLzp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sweep Neural Rock Typing - Train Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}