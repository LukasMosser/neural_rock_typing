{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-oxford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "institutional-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    os.system('git clone git@github.com:LukasMosser/neural_rock_typing.git')\n",
    "else:\n",
    "    print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-monitoring",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from neural_rock.dataset import ThinSectionDataset\n",
    "from neural_rock.utils import set_seed\n",
    "from neural_rock.model import NeuralRockModel\n",
    "from neural_rock.plot import visualize_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-impression",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intellectual-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelset = 'Dunham' # 'Lucia' 'DominantPore'\n",
    "\n",
    "learning_rate = 3e-4\n",
    "batch_size = 16\n",
    "weight_decay = 1e-5\n",
    "dropout = 0.5\n",
    "\n",
    "train_dataset_mult = 10\n",
    "val_dataset_mult = 50\n",
    "\n",
    "epochs = 100\n",
    "check_val_every = 10\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-nowhere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_name = 'lukas-mosser'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed, cudnn=True, benchmark=True)\n",
    "\n",
    "data_transforms = {\n",
    "    'train': A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(360, always_apply=True),\n",
    "            A.RandomCrop(width=512, height=512),\n",
    "            A.GaussNoise(),\n",
    "            A.HueSaturationValue(sat_shift_limit=0, val_shift_limit=50, hue_shift_limit=255, always_apply=True),\n",
    "            A.Resize(width=224, height=224),\n",
    "            A.Normalize()\n",
    "]),\n",
    "    'val': A.Compose([\n",
    "    A.RandomCrop(width=512, height=512),\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.Normalize(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "train_dataset_base = ThinSectionDataset(\"./data/Images_PhD_Miami/Leg194\", args.labelset,\n",
    "                                   transform=data_transforms['train'], train=True, seed=args.seed)\n",
    "val_dataset = ThinSectionDataset(\"./data/Images_PhD_Miami/Leg194\", args.labelset,\n",
    "                                 transform=data_transforms['val'], train=False, seed=args.seed)\n",
    "\n",
    "train_dataset = ConcatDataset([train_dataset_base]*train_dataset_mult)\n",
    "val_dataset = ConcatDataset([val_dataset]*val_dataset_mult)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True, prefetch_factor=10)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True, prefetch_factor=10)\n",
    "\n",
    "if args.plot:\n",
    "    visualize_batch(train_loader)\n",
    "    visualize_batch(val_loader)\n",
    "\n",
    "wandb_logger = WandbLogger(name=wandb_name, project='neural-rock')\n",
    "tensorboard_logger = TensorBoardLogger(\"lightning_logs\", name=labelset)\n",
    "checkpointer = ModelCheckpoint(monitor=\"val/f1\", verbose=True, mode=\"max\")\n",
    "trainer = pl.Trainer(gpus=-1, max_epochs=epochs, benchmark=True,\n",
    "                     logger=[wandb_logger, tensorboard_logger],\n",
    "                     callbacks=[checkpointer],\n",
    "                     check_val_every_n_epoch=10)\n",
    "\n",
    "model = NeuralRockModel(num_classes=len(train_dataset_base.class_names))\n",
    "\n",
    "trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-neural_rock]",
   "language": "python",
   "name": "conda-env-.conda-neural_rock-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
